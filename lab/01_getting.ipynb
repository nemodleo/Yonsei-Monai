{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "## Getting Started with MONAI\n",
    "\n",
    "![monai logo](https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png)\n",
    "\n",
    "Welcome to the MONAI bootcamp! This notebook will introduce you to an end-to-end workin in MONAI using both a standard PyTorch loop and using Ignite, followed by a hands-on implementing your own segmentation using everything you've learned so far.\n",
    "\n",
    "#### Required Packages \n",
    "\n",
    "Execute the following cell to install MONAI the first time a colab notebook is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLtpMbJVNCH",
    "outputId": "a0e88869-2f92-4f3b-d5b9-34db0cdfdbed"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.8.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU Support\n",
    "\n",
    " Running\n",
    "\n",
    "`!nvidia-smi`\n",
    "\n",
    "in a cell will verify this has worked and show you what kind of hardware you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "d121896f-b2c4-4ed6-f806-a433b097199b"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnPczYfDXV3K"
   },
   "source": [
    "# Getting Started with MONAI\n",
    "\n",
    "MONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem.\n",
    "\n",
    "Its ambitions are:\n",
    "\n",
    "developing a community of academic, industrial and clinical researchers collaborating on a common foundation;\n",
    "creating state-of-the-art, end-to-end training workflows for healthcare imaging;\n",
    "providing researchers with the optimized and standardized way to create and evaluate deep learning models.\n",
    "MONAI aims at supporting deep learning in medical image analysis at multiple granularities. This figure shows a typical example of the end-to-end workflow in medical deep learning area:\n",
    "\n",
    "![monai pipeline](https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/end_to_end.png)\n",
    "\n",
    "\n",
    "# What's the Need?\n",
    "Biomedical applications have specific requirements\n",
    "Image modalities (MR, CT, US, etc.) require specific data processing\n",
    "Data formats (DICOM, NIfTI, etc.) are specific to medical applications and require special support\n",
    "Certain network architectures are designed for, or are highly suitable for, biomedical applications\n",
    "Data transforms specific to biomedical applications, and to image modalities, are very useful when pre-processing data, augmenting data during training, and post-processing\n",
    "Reproducible science requires reproducible experiments which in turn rely on software accessible to other scientists, even if just as a common baseline\n",
    "A community-driven library to provide solutions to these requirements reduces duplication/re-implementation\n",
    "Baseline implementations of common networks, and implementations of networks specific to certain papers, provides a basis for comparison between other's networks and results\n",
    "How Does MONAI Address This Need?\n",
    "MONAI provides a framework of deep learning facilities and infrastructure to meet these needs in a flexible Pytorch-compatible way:\n",
    "\n",
    "Data loading and handling library for biomedical file types\n",
    "Large set of data transforms to process, regularize, and augment image date before, during, and after training\n",
    "Library of general-purpose network, metric, and loss function definitions implementing common architectures\n",
    "Set of ready-made components for training and inference to utilize computing infrastructure efficiently\n",
    "MONAI contributes to reproducibilty by making the training and distribution of experiments easier:\n",
    "\n",
    "Common underlying framework betweeen different users' implementations aids in comparison and interoperability\n",
    "Users can pick and choose components and existing networks then contribute new code to make their results available to others\n",
    "Reference implementations of networks from papers provides baselines for replication studies and comparisons\n",
    "\n",
    "# MONAI architecture\n",
    "\n",
    "The design principle of MONAI is to provide flexible and light APIs for users with varying expertise. All the core components are independent modules, which can be easily integrated into any existing PyTorch programs. Users can leverage the workflows in MONAI to quickly set up a robust training or evaluation program for research experiments. Rich examples and demos are provided to demonstrate the key features. Researchers contribute implementations based on the state-of-the-art for the latest research challenges, including COVID-19 image analysis, Model Parallel, etc.\n",
    "\n",
    "\n",
    "![monai architecture](https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/arch_modules_v0.4.png)\n",
    "\n",
    "# MONAI Design Philosophy\n",
    "\n",
    "Key principles:\n",
    "\n",
    "- MONAI looks like PyTorch, uses/extends PyTorch types and adheres to it's general design philosophy\n",
    "- MONAI is additive on top of PyTorch, providing extensions or wrappers\n",
    "- MONAI is opt-in and incremental, no need to rewrite entire models to integrate existing code\n",
    "- MONAI is collaborative, providing adapters and loosely coupled components to ease integration with third party code\n",
    "- MONAI is PyTorch ecosystem friendly, and part of the official ecosystem!\n",
    "\n",
    "# MONAI Transforms\n",
    "To help you understand more about MONAI transforms, this guide will help you answer five key questions:\n",
    "\n",
    " 1. What transforms are available to help create a data pipeline for training?\n",
    " 2. What are array transforms?\n",
    " 3. What is required to write a custom transform?\n",
    " 4. What are dictionary transforms?\n",
    " 5. How do I create a basic MONAI dataset with transforms?\n",
    "\n",
    "Let's get started by importing our dependecies. We're going to load everything that we'll need for the remainder of the notebook here. You'll see a lot of import statements, but we'll make sure to go over each of them throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn6Onix9XyxR",
    "outputId": "e12ac284-bf7f-426a-bcc8-0abc497cc0b3"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Any, Mapping, Hashable\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.utils import first\n",
    "from monai.config import KeysCollection\n",
    "from monai.data import Dataset, ArrayDataset, create_test_image_3d, DataLoader\n",
    "from monai.transforms import (\n",
    "    Transform,\n",
    "    MapTransform,\n",
    "    Randomizable,\n",
    "    AddChannel,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    LoadImaged,\n",
    "    Lambda,\n",
    "    Lambdad,\n",
    "    RandSpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    ToTensor,\n",
    "    ToTensord,\n",
    "    Orientation, \n",
    "    Rotate\n",
    ")\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLsN8dlfX94a"
   },
   "source": [
    "\n",
    "# 1. What transforms are available to help create a data pipeline for training?\n",
    "\n",
    "## Medical image data I/O, processing and augmentation\n",
    "\n",
    "Medical images require highly specialized methods for I/O, preprocessing, and augmentation. Medical images are often in specialized formats with rich meta-information, and the data volumes are often high-dimensional. These require carefully designed manipulation procedures. The medical imaging focus of MONAI is enabled by powerful and flexible image transformations that facilitate user-friendly, reproducible, optimized medical data pre-processing pipelines.\n",
    "\n",
    "![seg](https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/medical_transforms.png)\n",
    "\n",
    "## Transforms support both Dictionary and Array format data\n",
    "\n",
    "The widely used computer vision packages (such as torchvision) focus on spatially 2D array image processing. MONAI provides more domain-specific transformations for both spatially 2D and 3D and retains the flexible transformation “compose” feature.\n",
    "\n",
    "As medical image preprocessing often requires additional fine-grained system parameters, MONAI provides transforms for input data encapsulated in python dictionaries. Users can specify the keys corresponding to the expected data fields and system parameters to compose complex transformations.\n",
    "\n",
    "There is a rich set of transforms in six categories: Crop & Pad, Intensity, IO, Post-processing, Spatial, and Utilities. For more details, please visit all the transforms in MONAI.\n",
    "\n",
    "## Medical specific transforms\n",
    "\n",
    "MONAI aims at providing a comprehensive medical image specific transformations. These currently include, for example:\n",
    "\n",
    " - LoadImage: Load medical specific formats file from provided path\n",
    " - Spacing: Resample input image into the specified pixdim\n",
    " - Orientation: Change the image’s orientation into the specified axcodes\n",
    " - RandGaussianNoise: Perturb image intensities by adding statistical noises\n",
    " - NormalizeIntensity: Intensity Normalization based on mean and standard deviation\n",
    " - Affine: Transform image based on the affine parameters\n",
    " - Rand2DElastic: Random elastic deformation and affine in 2D\n",
    " - Rand3DElastic: Random elastic deformation and affine in 3D\n",
    "\n",
    "We'll create a temporary directory and populate it with a few example Nifti file-format images containing a random assortment of spheres. We're also creating a matching segmentation pair that will be used later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwl23rM4YTsj"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fn_keys = (\"img\", \"seg\")  # filename keys for image and seg files\n",
    "\n",
    "root_dir = tempfile.mkdtemp()\n",
    "filenames = []\n",
    "\n",
    "for i in range(5):\n",
    "    im, seg = create_test_image_3d(256, 256, 256, num_objs=25, rad_max=50)\n",
    "\n",
    "    im_filename = f\"{root_dir}/im{i}.nii.gz\"\n",
    "    seg_filename = f\"{root_dir}/seg{i}.nii.gz\"\n",
    "    filenames.append({\"img\": im_filename, \"seg\": seg_filename})\n",
    "\n",
    "    n = nib.Nifti1Image(im, np.eye(4))\n",
    "    nib.save(n, im_filename)\n",
    "\n",
    "    n = nib.Nifti1Image(seg, np.eye(4))\n",
    "    nib.save(n, seg_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk0vi85vYSZv"
   },
   "source": [
    "## 2. What are array transforms?\n",
    "\n",
    "Transforms in MONAI are callable objects accepting inputs from initial data in a dataset or previous transforms. We can create and call these directly without any infrastructure or system setup as components in MONAI are designed to be as decoupled as possible. For example we can load one of our Nifti files directly by creating the transform and calling it.\n",
    "\n",
    "Transforms are composed with Compose to create a sequence of operations. Compose itself being a transform we can also call it directly. The type of img here is numpy.ndarray so to convert to a Pytorch tensor as part of a training data pipeline we'd have ToTensor as the last transform in our sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ny9hsGAQYY6P",
    "outputId": "44697e44-25dc-4bfe-fae1-634d9415290b"
   },
   "outputs": [],
   "source": [
    "trans = Compose([LoadImage(image_only=True), AddChannel(), ToTensor()])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "print(type(img), img.shape, img.get_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9U7JgypYZDv"
   },
   "source": [
    "## 3. How do I create a custom transform?\n",
    "\n",
    "We can define our own custom transform operation in a number of ways. If a simple callable is used as an operator, Lambda can be used to wrap it as a transform. We define in this example a transform to sum the image in the 1st (width) dimension to produce a 2D image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "5fsL7CwPYcva",
    "outputId": "081011a7-f0e4-4f6a-b75e-e6b234d7f24a"
   },
   "outputs": [],
   "source": [
    "def sum_width(img):\n",
    "    return img.sum(1)\n",
    "\n",
    "trans = Compose([LoadImage(image_only=True), AddChannel(), Lambda(sum_width)])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivh-PSTLYfnc"
   },
   "source": [
    "Creating a subclass of Transform is the second method, and this has the advantage of being able to define attributes with the instantiated objects. Let's define a class to sum in a chosen dimension, and use it to sum in the 2nd (height) dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "Xqitlet2Yg3j",
    "outputId": "7485e734-30aa-4c5e-933f-0c33816de64e"
   },
   "outputs": [],
   "source": [
    "class SumDimension(Transform):\n",
    "    def __init__(self, dim=1):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs.sum(self.dim)\n",
    "\n",
    "trans = Compose([LoadImage(image_only=True), AddChannel(), SumDimension(2)])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt54UcP4Yi8K"
   },
   "source": [
    "All of these example transforms so far have been deterministic, to define transforms which perform some stochastic operation on input data we want to also inherit from `Randomizable`. This class is used to randomize variables but also distinguish from deterministic transforms. We'll see why this is important later in caching data loaders.\n",
    "\n",
    "In this class we have a `numpy.random.RandomState` object to provide stochastic values. This can be replaced using `Randomizable.set_random_state()` to control the randomization process. The `randomize()` method is responsible for determining if the random operation is to be performed based on the `prob` probability member, then creates the random noise array if so. This functionality is in this method so that it can be called by `Compose` or other external controllers.\n",
    "\n",
    "For now lets define a simple transform to add noise.\n",
    "\n",
    "<b>Run this cell a few times to see the random transform being applied 50% of the time.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "wWNf7ixMYwKP",
    "outputId": "d63674b8-a47f-46a4-f86e-4190c7f30c6d"
   },
   "outputs": [],
   "source": [
    "class RandAdditiveNoise(Randomizable, Transform):\n",
    "    def __init__(self, prob: float = 0.5, max_add: float = 1.0) -> None:\n",
    "        self.prob = np.clip(prob, 0.0, 1.0)\n",
    "        self.max_add = max_add\n",
    "        self._noise = 0\n",
    "\n",
    "    def randomize(self, data: np.ndarray) -> None:\n",
    "        self._noise = 0\n",
    "\n",
    "        if self.R.random() < self.prob:\n",
    "            noise_array = self.R.rand(*data.shape[1:])[None]\n",
    "            self._noise = (noise_array * self.max_add).astype(data.dtype)\n",
    "\n",
    "    def add_noise(self, img: np.ndarray) -> np.ndarray:\n",
    "        return img + self._noise\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        self.randomize(img)\n",
    "        return self.add_noise(img)\n",
    "\n",
    "trans = Compose([LoadImage(image_only=True), AddChannel(), RandAdditiveNoise()])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "plt.imshow(img[0, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkyLXyGgY0-c"
   },
   "source": [
    "## 4. What are dictionary transforms?\n",
    "\n",
    "So far we have seen transforms which are applied to individual Numpy arrays, however for most training schemes a pipeline with multiple values is needed. To address this MONAI includes transforms for operating on dictionaries of arrays, one for each equivalent array transform. These can be applied to named values in an input dictionary while leaving unnamed values untouched, for example adding noise to an image while leaving the associated label image untouched.\n",
    "\n",
    "Earlier in the notebook we imported the dictionary equivalent transforms which have a d appended to their names, we'll use those transforms in this section. The `keys` argument in `LoadNiftid` is used to state which keys contain paths to Nifti files, all other values in the input dictionary will be retained. With this set we can look at the keys returned when calling the transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe4-deuBY3_s",
    "outputId": "63334a80-246f-47c0-b7d1-7b7a477de196"
   },
   "outputs": [],
   "source": [
    "trans = LoadImaged(keys=fn_keys)\n",
    "data = trans(filenames[0])\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROolxTGMY5dQ"
   },
   "source": [
    "`Lambdad` applies the given callable to each array named by `keys` separately. We can use this to define transforms operating on different named values in the dictionary at different points in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "BxkYtbBMZBQx",
    "outputId": "14f435b6-1648-4902-e405-85164d5bf6fb"
   },
   "outputs": [],
   "source": [
    "def sum_width(img):\n",
    "    return img.sum(1)\n",
    "\n",
    "def max_width(img):\n",
    "    return img.max(1)\n",
    "\n",
    "trans = Compose([LoadImaged(fn_keys), AddChanneld(fn_keys), Lambdad((\"img\",), sum_width), \n",
    "                 Lambdad((\"seg\",), max_width)])\n",
    "\n",
    "imgd = trans(filenames[0])\n",
    "img = imgd[\"img\"]\n",
    "seg = imgd[\"seg\"]\n",
    "\n",
    "plt.imshow(np.hstack((img[0] * 5 / img.max(), seg[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aM3F4fMZEMG"
   },
   "source": [
    "The above applies one operation to one member of the dictionary and different operation to another. A reasonable re-implementation of this in one transform would require retaining the names of which members to apply which transform to and applying the operations in one method. Adapting array-based transforms to operate over dictionaries is relatively straight-forward.\n",
    "\n",
    "<b>Run this cell a few times to see the random transform being applied 50% of the time.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "uGyqW-QEZGAL",
    "outputId": "90bc64be-9f29-45a7-a83a-aa94f9e9d5db"
   },
   "outputs": [],
   "source": [
    "class RandAdditiveNoised(Randomizable, MapTransform):\n",
    "    def __init__(\n",
    "        self, keys: KeysCollection, prob: float = 0.5, max_add: float = 1.0\n",
    "    ) -> None:\n",
    "        super(Randomizable, self).__init__(keys)\n",
    "        self.transform = RandAdditiveNoise(prob, max_add)\n",
    "\n",
    "    def set_random_state(\n",
    "        self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None\n",
    "    ) -> \"RandAdditiveNoised\":\n",
    "        self.transform.set_random_state(seed, state)\n",
    "        super().set_random_state(seed, state)\n",
    "        return self\n",
    "\n",
    "    def randomize(self, data: Optional[Any] = None) -> None:\n",
    "        self.transform.randomize(data)\n",
    "\n",
    "    def __call__(\n",
    "        self, data: Mapping[Hashable, np.ndarray]\n",
    "    ) -> Mapping[Hashable, np.ndarray]:\n",
    "        self.randomize(data[monai.utils.first(self.keys)])\n",
    "\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.transform.add_noise(d[key])\n",
    "        return d\n",
    "\n",
    "trans = Compose([LoadImaged(fn_keys), AddChanneld(fn_keys), RandAdditiveNoised((\"img\",))])\n",
    "img = trans(filenames[0])\n",
    "\n",
    "# We're adding random noise to the image, not the segmentation\n",
    "plt.imshow(np.hstack([img[\"img\"][0, 50], img[\"seg\"][0, 50]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv6DfUxiZJqI"
   },
   "source": [
    "Method overrides are provided to delegate behaviour to an internal `RandAdditiveNoise` instance:\n",
    "\n",
    " - `set_random_state` sets the state of the delegate and the current object\n",
    " - `randomize` delegates the randomization to the `RandAdditiveNoise` instance\n",
    " - `__call__` causes the delegate to randomize then applies the transform to each named member of the dictionary. The delegate transform is randomized only once, this ensures the same random field is added to each named member of the dictionary, a slightly different implementation adding a per-key random field would be needed if this were the desired behaviour.\n",
    "\n",
    " ## 5. How do I create a basic MONAI dataset with transforms?\n",
    " \n",
    "Now that we've taken a look at transform, let's take a look at datasets. With a data source and transforms defined we can now create a dataset object. The base class for MONAI is `Dataset`, created here to load the image Nifti files only.\n",
    "\n",
    "`Dataset` inherits from the Pytorch class of that name and adds only the ability to apply the given transform to selected items. If you're familiar with the class from Pytorch this will work the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDaZuVLWZbaA",
    "outputId": "27cd5056-eac2-44ce-ca7d-f673ebdc246f"
   },
   "outputs": [],
   "source": [
    "images = [fn[\"img\"] for fn in filenames]\n",
    "\n",
    "transform = Compose([LoadImage(image_only=True), AddChannel(), ToTensor()])\n",
    "ds = Dataset(images, transform)\n",
    "img_tensor = ds[0]\n",
    "print(img_tensor.shape, img_tensor.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As3vFVltZc4Y"
   },
   "source": [
    "MONAI provides the `ArrayDataset` for supervised training applications specifically. It can accept data arrays for images separate from those for segmentations or labels with their own separate transforms. Here we will again separate out the image and segmentation filenames to demonstrate this usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "72Pt6mTvZgFn",
    "outputId": "6ebf92b6-2b38-4189-b16d-cbe3a5ebc913"
   },
   "outputs": [],
   "source": [
    "images = [fn[\"img\"] for fn in filenames]\n",
    "segs = [fn[\"seg\"] for fn in filenames]\n",
    "\n",
    "img_transform = Compose([LoadImage(image_only=True), AddChannel(), \n",
    "                         RandSpatialCrop((128, 128, 128), random_size=False), RandAdditiveNoise(), ToTensor()])\n",
    "seg_transform = Compose([LoadImage(image_only=True), AddChannel(), \n",
    "                         RandSpatialCrop((128, 128, 128), random_size=False), ToTensor()])\n",
    "\n",
    "ds = ArrayDataset(images, img_transform, segs, seg_transform)\n",
    "im, seg = ds[0]\n",
    "plt.imshow(np.hstack([im.numpy()[0, 48], seg.numpy()[0, 48]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WphXUUTFZkCD"
   },
   "source": [
    "nstead of returning a single image, accessing a member of the dataset produces a pair containing the image and segmentation after being pass through their respective transforms. One important aspect of this class is that the random state of each transform (`Compose` in this case) is set to that of the dataset before being applied. This ensures the same random operations are applied to each output, which is why the `RandSpatialCrop` operation chooses the same crop window for the image as well as the segmentation. By having separate transforms one can apply operations to images and not to segmentations (or vice versa), being careful that these unshared operations come after the shared ones.\n",
    "\n",
    "Alternatively, `Dataset` can be used with dictionary-based transforms to construct a result mapping. For training applications beyond simple input/ground-truth pairs like the above this would be more suitable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "i4uohRiXZrew",
    "outputId": "4351a122-a0e0-4699-fb16-bd56876aa394"
   },
   "outputs": [],
   "source": [
    "trans = Compose([LoadImaged(fn_keys), AddChanneld(fn_keys), RandAdditiveNoised((\"img\",)), \n",
    "                 RandSpatialCropd(fn_keys, (128, 128, 128), random_size=False), ToTensord(fn_keys)])\n",
    "\n",
    "ds = Dataset(filenames, trans)\n",
    "item = ds[0]\n",
    "im, seg = item[\"img\"], item[\"seg\"]\n",
    "plt.imshow(np.hstack([im.numpy()[0, 48], seg.numpy()[0, 48]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBiXS6QZZwNW"
   },
   "source": [
    "With the dataset defined, we can now create the dataloader to create data batches. This inherits directly from Pytorch's `DataLoader` class with a few changes to the default constructor arguments. MONAI functionality should be compatible with the PyTorch `DataLoader`, but it was subclasses to include additional functionality that we consider key and which cannot be realized with the standard `DataLoader` class.\n",
    "\n",
    "The `DataLoader` will use five worker processes to load the actual data. MONAI provides a number of `Dataset` subclasses to improve the efficiency of this process. These and other features will be covered in subsequent labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "b1VGpYB4ZwWC",
    "outputId": "a912b5c8-0143-42e8-cffc-4cb998e2268d"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(ds, batch_size=5, num_workers=0)\n",
    "batch = first(loader)\n",
    "print(list(batch.keys()), batch[\"img\"].shape)\n",
    "\n",
    "f, ax = plt.subplots(2, 1, figsize=(8, 4))\n",
    "ax[0].imshow(np.hstack(batch[\"img\"][:, 0, 64]))\n",
    "ax[1].imshow(np.hstack(batch[\"seg\"][:, 0, 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnXoOD5GZweI"
   },
   "source": [
    "# Summary\n",
    "We've covered MONAI Transforms. Some key highlights are:\n",
    "\n",
    "There is a long list of medical specific transforms available in MONAI\n",
    "There are array and dictionary versions of transforms.\n",
    "You can create a simple callable lambda function or create a class based on transform to create your own custom tranform\n",
    "You can create a MONAI dataset and directly pass a compose tranform chain to it\n",
    "\n",
    "# Assignment 1\n",
    "\n",
    "Now you get to practice creating a transform of your own and integrating it into a compose array.\n",
    "\n",
    "Create a transform which selects a patch from an input image of a given size and randomly chooses to transpose this data along randomly chosen axes. For example, this could produce an image where a patch in the center has been flipped in the horizontal direction while the rest of the image is left unchanged.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    " - Implement this as an array randomized transform\n",
    " - Use a default probability ratio of 40% chance to crop\n",
    " - Use one of the test 3D images from above to validate\n",
    " - Default crop size is 150 but allow it to be variable\n",
    " - Don't worry about edge cases\n",
    "\n",
    "Below is the initial class definition you'll need to implement. We also include a compose chain and image visualization call that should work once your function is fully implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9GUx3-gZwmu"
   },
   "outputs": [],
   "source": [
    "class RandFlipRect(Randomizable, Transform):\n",
    "    \"\"\"\n",
    "    Define a transform to randomly transpose a patch of the input image along randomly chosen spatial dimensions. This \n",
    "    uses arguments `prob` for the probability the transform is applied, and `size` giving the size of the cube to flip.\n",
    "    \"\"\"\n",
    "    def __init__(self, prob: float = 0.4, size: int = 100) -> None:\n",
    "        self.prob = np.clip(prob, 0.0, 1.0)\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        pass # TODO replace this with your code\n",
    "    \n",
    "trans = Compose([LoadImage(image_only=True), AddChannel(), RandFlipRect()])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "plt.imshow(img[0, img.shape[1]//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2lxbmMdbP1M"
   },
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "9ZP3QbETbU3h",
    "outputId": "31f78f89-4c4f-4cf0-eb2b-0567c27f46f0"
   },
   "outputs": [],
   "source": [
    "#@title solution for RandFlipRect\n",
    "class RandFlipRect(Randomizable, Transform):\n",
    "    \"\"\"\n",
    "    Define a transform to randomly transpose a patch of the input image along randomly chosen spatial dimensions. This \n",
    "    uses arguments `prob` for the probability the transform is applied, and `size` giving the size of the cube to flip.\n",
    "    \"\"\"\n",
    "    def __init__(self, prob: float = 0.4, size: int = 100) -> None:\n",
    "        self.prob = np.clip(prob, 0.0, 1.0)\n",
    "        self.size = size\n",
    "\n",
    "    def random_flip(self, data: np.ndarray) -> None:\n",
    "        if self.R.random() < self.prob:\n",
    "            height = data.shape[1]\n",
    "            width = data.shape[2]\n",
    "            depth = data.shape[3]\n",
    "            \n",
    "            x = self.R.randint(0, height - self.size)\n",
    "            y = self.R.randint(0, width - self.size)\n",
    "            z = self.R.randint(0, depth - self.size)\n",
    "            patch = data[..., x: x + self.size , y: y + self.size, z: z + self.size]\n",
    "            \n",
    "            if self.R.random() < 0.333:\n",
    "                patch=patch[...,::-1,:,:]\n",
    "                \n",
    "            if self.R.random() < 0.333:\n",
    "                patch=patch[...,::-1,:]\n",
    "                \n",
    "            if self.R.random() < 0.333:\n",
    "                patch=patch[...,::-1]\n",
    "                \n",
    "            data[..., x: x + self.size , y: y + self.size, z: z + self.size]=patch\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        return self.random_flip(img)\n",
    "    \n",
    "trans = Compose([LoadImage(image_only=True), AddChannel(), RandFlipRect()])\n",
    "img = trans(filenames[0][\"img\"])\n",
    "plt.imshow(img[0, img.shape[1]//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhhycblyZwux"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "In the cell below define a dictionary-based transform pipeline which will take input images and segmentations and ensure:\n",
    "\n",
    " - both image and segmentation arrays have a channel dimension\n",
    " - each image/segmentation pair is 200x200x200 pixels in shape\n",
    " - the value range of each image is between 0 and 1\n",
    " - the dtype of the image is float32 and that of the segmentation is int32\n",
    " - both are produced in the end as tensors\n",
    "\n",
    "All this can be done with built-in transforms so you shouldn't need a lambda transform. Once your pipeline works, use it to construct and dataset and then a data loader. Iterate over every value in the data loader and check that the image and segmentation members of the produced dictionaries meet these requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIRWofg1Zw29"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.transforms import LoadImaged, ToTensord\n",
    "\n",
    "trans = Compose(\n",
    "    [\n",
    "        # ???  TODO \n",
    "    ]\n",
    ")\n",
    "\n",
    "data = trans(filenames[0])\n",
    "img = data[\"img\"]\n",
    "seg = data[\"seg\"]\n",
    "\n",
    "print(img.shape)  # should be (1, 200, 200, 200)\n",
    "print(img.dtype, seg.dtype)  # should be float32  int32\n",
    "print(img.min(), img.max())  # should be 0.0 1.0\n",
    "print(torch.unique(seg))  # should be [0 1 2 3 4 5]\n",
    "\n",
    "plt.imshow(img[0, img.shape[1] // 2])\n",
    "\n",
    "# construct dataset and dataloder here and iterate over batches, batch_size of 1 should be used\n",
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD1j_1HGbaob"
   },
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "p6ap_daGbcD8",
    "outputId": "1d1efa35-5b3f-4792-b67c-7c0b9757c0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200, 200, 200])\n",
      "torch.float32 torch.int32\n",
      "tensor(0.) tensor(1.)\n",
      "tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title solution for dataloader\n",
    "import torch\n",
    "from monai.transforms import LoadImaged, CenterSpatialCropd, AddChanneld, ScaleIntensityd, CastToTyped, ToTensord\n",
    "\n",
    "trans = Compose(\n",
    "    [\n",
    "        LoadImaged(fn_keys, image_only=True),\n",
    "        AddChanneld(fn_keys),\n",
    "        CenterSpatialCropd(fn_keys, 200),\n",
    "        ScaleIntensityd([\"img\"]),\n",
    "        CastToTyped(fn_keys, (np.float32, np.int32)),\n",
    "        ToTensord(fn_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = trans(filenames[0])\n",
    "img = data[\"img\"]\n",
    "seg = data[\"seg\"]\n",
    "\n",
    "print(img.shape)  # should be (1, 200, 200, 200)\n",
    "print(img.dtype, seg.dtype)  # should be float32  int32\n",
    "print(img.min(), img.max())  # should be 0.0 1.0\n",
    "print(torch.unique(seg))  # should be [0 1 2 3 4 5]\n",
    "\n",
    "plt.imshow(img[0, img.shape[1] // 2])\n",
    "\n",
    "# construct dataset and dataloder here and iterate over batches, batch_size of 1 should be used\n",
    "\n",
    "ds = Dataset(filenames,transform=trans)\n",
    "loader= DataLoader(ds,batch_size=3)\n",
    "\n",
    "for batch in loader:\n",
    "    img = batch[\"img\"]\n",
    "    seg = batch[\"seg\"]\n",
    "\n",
    "    print(img.shape)  # should be (3, 1, 200, 200, 200) and (2, 1, 200, 200, 200) - first batch has 3, second has 2.\n",
    "    print(img.dtype, seg.dtype)  # should be float32  int32\n",
    "    print(img.min(), img.max())  # should be 0.0 1.0\n",
    "    print(torch.unique(seg))  # should be [0 1 2 3 4 5]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEypDbVWbjb6"
   },
   "source": [
    "\n",
    "\n",
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Monai_bootcam_01_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
