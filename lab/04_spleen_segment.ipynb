{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "## Spleen Segmentation\n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>\n",
    "\n",
    "In this notebook, we will implement Spleen segmentation using the Medical Decathalon dataset. We'll follow these general steps:\n",
    "\n",
    "- Transforms for dictionary format data.\n",
    "- Load Nifti image with metadata.\n",
    "- Add channel dim to the data if no channel dimension.\n",
    "- Scale medical image intensity with expected range.\n",
    "- Crop out a batch of balanced images based on positive / negative label ratio.\n",
    "- Cache IO and transforms to accelerate training and validation.\n",
    "- 3D UNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "- Sliding window inference method.\n",
    "- Deterministic training for reproducibility.\n",
    "- Optionally replace the training loop with the Ignite-derived engine classes.\n",
    "\n",
    "#### Required Packages \n",
    "This notebook has the pip command for installing MONAI and will be added to any subsequent notebook.\n",
    "Execute the following cell to install MONAI the first time a colab notebook is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLtpMbJVNCH",
    "outputId": "40da1e90-8167-45af-e4ca-d783eef3029c"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.8.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU Support\n",
    "\n",
    "Running `!nvidia-smi`\n",
    "\n",
    "in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
    "if GPU Memory Usage is no `0 MiB` shutdown all kernels and restart current kernel.\n",
    "- step1. shutdown kernel with following <b>Menu</b> > <b>Kernel</b> > <b>Shut Down All kernels </b>\n",
    "- step2. restart kernelw with following <b>Menu</b> > <b>Kernel</b> > <b>Restart Kernel</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "2783c3a4-855a-4b2b-86c0-297b7b31b9ce"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX4DkxZygKNP"
   },
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNnKTuzBgKZm",
    "outputId": "8b7c3607-768e-4222-d4c0-82806f5a1264"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()\n",
    "set_determinism(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "directory = \"temp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNZjqg3Lq-1"
   },
   "source": [
    "## download dataset \n",
    "\n",
    "it would take 1 minutes to download spleen dataset(1.5GB). You would also use cached dataset\n",
    "\n",
    "You can check Medical Segmentation Decathlon dataset [homepage](http://medicaldecathlon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import glob\n",
    "import os\n",
    "\n",
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5-WItNULaj8",
    "outputId": "8c2a4e8f-53f9-42d5-e72c-c80aaac26131"
   },
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(train_images, train_labels)\n",
    "]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQ9g2N2mxfb"
   },
   "source": [
    "### visualize dataset\n",
    "Let's use the nibabel library to visualize and examine the Spleen data in the form of a compressed file `nii.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJs0Jl5pOerR",
    "outputId": "4eef4dc0-9ea8-44a4-a2ad-acac973bfd33"
   },
   "outputs": [],
   "source": [
    "#!pip install SimpleITK   nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAZuobWLL8a9",
    "outputId": "01c47ccc-d23e-4c99-b49d-0a386dd41406"
   },
   "outputs": [],
   "source": [
    "val_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhvAlLBlO8Kb"
   },
   "outputs": [],
   "source": [
    "def nii_loader(filename) :\n",
    "    import nibabel as nib\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    nimg = nib.load( filename )\n",
    "    return nimg.get_fdata() , nimg.affine, nimg.header \n",
    "\n",
    "def visualize( dataset, idx=0, target_layer=10 ):\n",
    "    import matplotlib.pyplot as plt    \n",
    "\n",
    "    image = dataset[idx]['image']\n",
    "    label = dataset[idx]['label']\n",
    "\n",
    "    image_data, image_affine, image_header = nii_loader(image)\n",
    "    label_data, label_affine,   label_header = nii_loader(label)\n",
    "\n",
    "    print(image_data.shape, label_data.shape )\n",
    "    target_image = image_data[:,:,target_layer]\n",
    "    target_label = label_data[:,:,target_layer]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2 ,  figsize=(12,8))\n",
    "\n",
    "    ax1.imshow(target_image, cmap='gray' )\n",
    "    ax1.set_title('image')\n",
    "    ax2.imshow(target_label )\n",
    "    ax2.set_title('GT segmentation')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "GHtMGH2MR4KF",
    "outputId": "6dd5a393-b74e-43dc-a79c-6314742a579a"
   },
   "outputs": [],
   "source": [
    "visualize( val_files, idx=8,  target_layer=30 ) # check different idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXrBwbrraiGl"
   },
   "source": [
    "## MONAI Dataloader\n",
    "### transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFFNQYNCLeqP"
   },
   "outputs": [],
   "source": [
    "from monai.transforms import Compose\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        #???\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        #???\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform for CT dataset\n",
    " - random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO410h13atLP"
   },
   "outputs": [],
   "source": [
    "from monai.transforms import Compose\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        # user can also add other random transforms\n",
    "        # RandAffined(\n",
    "        #     keys=['image', 'label'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     prob=1.0, spatial_size=(96, 96, 96),\n",
    "        #     rotate_range=(0, 0, np.pi/15),\n",
    "        #     scale_range=(0.1, 0.1, 0.1)),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "gKnvWOvTazu-",
    "outputId": "1330aba5-c5e5-4de5-e39a-5cfb9abb11cf"
   },
   "outputs": [],
   "source": [
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "# plot the slice [:, :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:, :, 80], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI data cache\n",
    "\n",
    "<b>class monai.data.CacheDataset</b>(`data, transform, cache_num=9223372036854775807, cache_rate=1.0, num_workers=None, progress=True, copy_cache=True, as_contiguous=True, hash_as_key=False, hash_func=<function pickle_hashing>`)\n",
    "\n",
    "Dataset with cache mechanism that can load data and cache deterministic transforms’ result during training.\n",
    "\n",
    "By caching the results of non-random preprocessing transforms, it accelerates the training data pipeline. \n",
    "Users can set the cache rate or number of items to cache. It is recommended to experiment with different cache_num or cache_rate to identify the best training speed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It takes 2 minutes to load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoFLJK7mcUQE",
    "outputId": "4f76b110-9683-4e6d-ea0a-ae14589c013b"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from monai.data import CacheDataset, DataLoader\n",
    "\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4, progress=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=10)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4, progress=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monai Network\n",
    "MONAI provides predefined networks. we can easily import it. \n",
    "\n",
    "- [Layers](https://docs.monai.io/en/stable/networks.html#layers) : Act, Conv, Norm, Dropout, Flatten, Reshape, Pad, Pool, SkipConnection\n",
    "- [Blocks](https://docs.monai.io/en/stable/networks.html#module-monai.networks.blocks) : ADN, Convolution, Synamic UnetBlock, FCN, GCN, Squeeze-andExcitation, ResNeXt, SABlock, Transformer Block, \n",
    "- [Nets](https://docs.monai.io/en/stable/networks.html#nets)  : DenseNet121, EfficientNet, SegResNet, ResNet, SENet154, DyUNet, UNet, AutoEncoder, VarAutoEncoder, ViT, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obSCDT0F05zt"
   },
   "source": [
    "### UNet\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "U Shaped Network was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. \n",
    "It is Fully Convolutional Network Model for the segmentation task with two paths(encoder and decoder) with 1x1 convolution skip connection similar as residual. \n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\" width=800>\n",
    "\n",
    "#### UNet in monai.networks\n",
    "class monai.networks.nets.<b>UNet </b> (`spatial_dims, in_channels, out_channels, channels, strides, kernel_size=3, up_kernel_size=3, num_res_units=0, act='PRELU', norm='INSTANCE', dropout=0.0, bias=True, dimensions=None` )\n",
    "Enhanced version of <br>UNet</b> which has residual units implemented with the `ResidualUnit` class. The residual part uses a convolution to change the input dimensions to match the output dimensions if this is necessary but will use `nn.Identity` if not. Refer to: [Link](https://link.springer.com/chapter/10.1007/978-3-030-12029-0_40).\n",
    "\n",
    "Each layer of the network has a encode and decode path with a skip connection between them. Data in the encode path is downsampled using strided convolutions (if strides is given values greater than 1) and in the decode path upsampled using strided transpose convolutions. These down or up sampling operations occur at the beginning of each block rather than afterwards as is typical in <b>UNet</b> implementations.\n",
    "\n",
    "To further explain this consider the first example network given below. This network has 3 layers with strides of 2 for each of the middle layers (the last layer is the bottom connection which does not down/up sample). Input data to this network is immediately reduced in the spatial dimensions by a factor of 2 by the first convolution of the residual unit defining the first layer of the encode part. The last layer of the decode part will upsample its input (data from the previous layer concatenated with data from the skip connection) in the first convolution. this ensures the final output of the network has the same shape as the input.\n",
    "\n",
    "Padding values for the convolutions are chosen to ensure output sizes are even divisors/multiples of the input sizes if the strides value for a layer is a factor of the input sizes. A typical case is to use strides values of 2 and inputs that are multiples of powers of 2. An input can thus be downsampled evenly however many times its dimensions can be divided by 2, so for the example network inputs would have to have dimensions that are multiples of 4. In the second example network given below the input to the bottom layer will have shape `(1, 64, 15, 15)` for an input of shape `(1, 1, 240, 240)` demonstrating the input being reduced in size spatially by 2**4.\n",
    "\n",
    "##### Parameters\n",
    " - <b>spatial_dims </b>(`int`)  – number of spatial dimensions.\n",
    " - <b>in_channels </b>(`int`) – number of input channels.\n",
    " - <b>out_channels </b>(`int`) – number of output channels.\n",
    " - <b>channels</b> (`Sequence[int]`) – sequence of channels. Top block first. The length of channels should be no less than 2.\n",
    " - <b>strides</b> (`Sequence[int]`) – sequence of convolution strides. The length of stride should equal to len(channels) - 1.\n",
    " - <b>kernel_size</b> (`Union[Sequence[int], int]`) – convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>up_kernel_size</b> (`Union[Sequence[int], int]`) – upsampling convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>num_res_units</b>  (`int`)– number of residual units. Defaults to 0.\n",
    " - <b>act</b> (`Union[Tuple, str]`) – activation type and arguments. Defaults to PReLU.\n",
    " - <b>norm</b> (`Union[Tuple, str]`) – feature normalization type and arguments. Defaults to instance norm.\n",
    " - <b>dropout</b> (`float`) – dropout ratio. Defaults to no dropout.\n",
    " - <b>bias (`bool`)</b> – whether to have a bias term in convolution blocks. Defaults to True. According to Performance Tuning Guide, if a conv layer is directly followed by a batch norm layer, bias should be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOxfQaWZczTS"
   },
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Loss \n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*HuENmnLgplFLg7Xv\" width=400>\n",
    " \n",
    " #### Dice Loss in MONAI losses\n",
    " \n",
    " class monai.losses.<b>DiceLoss</b>(`include_background=True, to_onehot_y=False, sigmoid=False, softmax=False, other_act=None, squared_pred=False, jaccard=False, reduction=LossReduction.MEAN, smooth_nr=1e-05, smooth_dr=1e-05, batch=False`)\n",
    " Compute average Dice loss between two tensors. It can support both multi-classes and multi-labels tasks. The data input (`BNHW[D]` where N is number of classes) is compared with ground truth target (`BNHW[D]`).\n",
    "\n",
    "Note that axis N of input is expected to be logits or probabilities for each class, if passing logits as input, must set `sigmoid=True` or `softmax=True`, or specifying `other_act`. And the same axis of target can be 1 or N (one-hot format).\n",
    "\n",
    "The smooth_nr and smooth_dr parameters are values added to the intersection and union components of the inter-over-union calculation to smooth results respectively, these values should be small.\n",
    "\n",
    "The original paper: Milletari, F. et. al. (2016) V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation, 3DV, 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3_YwmG4cX29",
    "outputId": "ab6882d3-1a4d-4513-96cd-3c4384b96e45"
   },
   "outputs": [],
   "source": [
    "from monai.data.utils import decollate_batch\n",
    "from monai.transforms import EnsureType, AsDiscrete\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "max_epochs = 50\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=True, n_classes=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=True, n_classes=2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8tSD1oLcdVA",
    "outputId": "38f96c2f-dced-4be3-a39f-26cc2f3e9015",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import time \n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    tic_epoch = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if step % 5 ==0:\n",
    "            print(\n",
    "                f\"\\nepoch {epoch + 1}/{max_epochs}\"\n",
    "                f\" steps {step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"|train_loss: {loss.item():.4f}\", end='')\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    toc_epoch = time.time()\n",
    "    dur_epoch = toc_epoch - tic_epoch\n",
    "    print(f\" |avg loss: {epoch_loss:.4f} dur {dur_epoch:.1f}s \", end='')\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            #print( \"| Val Dice metric {:6.4f}\".format(metric) , end='')\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "\n",
    "            \n",
    "            metric_values.append(metric)\n",
    "            print(\n",
    "                f\"|DICE cur: {metric:.4f}\"\n",
    "                f\" old best : {best_metric:.4f} \"\n",
    "                f\"at {best_metric_epoch}\" , end=''            )\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"spleen_unet_model_best.pth\"))\n",
    "                \n",
    "                print(\"|best model saved\", end='')            \n",
    "\n",
    "    if (epoch+1) % 10 == 0: \n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"spleen_unet_model_{:04d}.pth\".format(epoch+1)))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(root_dir, \"spleen_unet_optim_{:04d}.pth\".format(epoch+1)))\n",
    "        print(\"|{}ep model saved\".format(epoch+1), end='')\n",
    "                            \n",
    "print(\n",
    "    f\"\\ntrain completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\"\n",
    "    f\" for {max_epochs} epoches\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aFNf6SweRmT"
   },
   "source": [
    "### check loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "NoL8LbHBcfhx",
    "outputId": "2bf2b4dc-5acc-4fd7-eb4f-57e8ae21cf9a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.ylim(0,1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_segment(model, roi_size, sw_batch_size, num_data=2):\n",
    "    with torch.no_grad():\n",
    "        for i, val_data in enumerate(val_loader):\n",
    "            roi_size = (160, 160, 160)\n",
    "            sw_batch_size = 4\n",
    "            val_outputs = sliding_window_inference(\n",
    "                val_data[\"image\"].to(device), roi_size, sw_batch_size, model\n",
    "            )\n",
    "            # plot the slice [:, :, 80]\n",
    "            plt.figure(\"check\", (18, 6))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title(f\"image {i}\")\n",
    "            plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title(f\"label {i}\")\n",
    "            plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title(f\"output {i}\")\n",
    "            plt.imshow(torch.argmax(\n",
    "                val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "            plt.show()\n",
    "            if i == num_data:\n",
    "                break                \n",
    "    \n",
    "def inference_segment_best(work_dir, model, prefix, roi_size, sw_batch_size, num_data=2):\n",
    "    import torch\n",
    "    model.load_state_dict(torch.load(os.path.join(work_dir, \"spleen_{}_model_best.pth\".format(prefix) )))\n",
    "    model.eval()\n",
    "    inference_segment(model, roi_size, sw_batch_size, num_data)\n",
    "\n",
    "def inference_segment_epoch(work_dir, model, prefix, roi_size, sw_batch_size, epoch=30, num_data=2):\n",
    "    import torch\n",
    "    model.load_state_dict(torch.load(os.path.join(work_dir, \"spleen_{}_model_{:04d}.pth\".format(prefix, epoch))))\n",
    "    model.eval()\n",
    "    inference_segment(model, roi_size, sw_batch_size, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_segment_best(root_dir, model,  'unet', roi_size, sw_batch_size, num_data=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48BLsYUgeoT3"
   },
   "source": [
    "## resume with pretrained checkpoint\n",
    "we could resume train. \n",
    "For inference, we needs only model parameters. However, we need not only model parameters but also optimizer parameters for resume train. for AMP(automatic mixed precision), we also need scaler. \n",
    "In `root_dir`, the model parameter is saved as `spleen_model_0050.pth` file name and the optim parameter is saved as `spleen_optim_0050.pth` file name. Both files are required to resume learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh temp/spleen*.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to examine both the Dice metric and the projectory of loss during initial learning and re-execution of learning, `best_metric`, `best_metric_epoch`, `epoch_loss_values` and `metric_values` were annotated to utilize the variables stored in the previous learning. All of these variables are necessary for re-learning after resetting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data.utils import decollate_batch\n",
    "from monai.transforms import EnsureType, AsDiscrete\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "max_epochs = 100 ## \n",
    "resume_epoch = 50\n",
    "val_interval = 2\n",
    "#best_metric = -1\n",
    "#best_metric_epoch = -1\n",
    "#epoch_loss_values = []\n",
    "#metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=True, n_classes=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=True, n_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix = 'unet'\n",
    "model.load_state_dict(    torch.load(os.path.join(root_dir, \"spleen_{}_model_{:04d}.pth\".format(prefix, resume_epoch))))\n",
    "optimizer.load_state_dict(torch.load(os.path.join(root_dir, \"spleen_{}_optim_{:04d}.pth\".format(prefix, resume_epoch))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "import time \n",
    "print(\"resume to train\")\n",
    "for epoch in range(resume_epoch, max_epochs):\n",
    "\n",
    "    tic_epoch = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if step % 5 ==0:\n",
    "            print(\n",
    "                f\"\\nepoch {epoch + 1}/{max_epochs}\"\n",
    "                f\" steps {step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"|train_loss: {loss.item():.4f}\", end='')\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    toc_epoch = time.time()\n",
    "    dur_epoch = toc_epoch - tic_epoch\n",
    "    print(f\" |avg loss: {epoch_loss:.4f} dur {dur_epoch:.1f}s \", end='')\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            #print( \"| Val Dice metric {:6.4f}\".format(metric) , end='')\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "\n",
    "            \n",
    "            metric_values.append(metric)\n",
    "            print(\n",
    "                f\"|DICE cur: {metric:.4f}\"\n",
    "                f\" old best : {best_metric:.4f} \"\n",
    "                f\"at {best_metric_epoch}\" , end=''            )\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"spleen_unet_model_best.pth\"))\n",
    "                \n",
    "                print(\"|best model saved\", end='')            \n",
    "\n",
    "    if (epoch+1) % 10 == 0: \n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"spleen_unet_model_{:04d}.pth\".format(epoch+1)))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(root_dir, \"spleen_unet_optim_{:04d}.pth\".format(epoch+1)))\n",
    "        print(\"|{}ep model saved\".format(epoch+1), end='')\n",
    "                            \n",
    "print(\n",
    "    f\"\\ntrain completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\"\n",
    "    f\" for {max_epochs} epoches\" )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.ylim(0,1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_segment_best(root_dir, model, 'unet', roi_size, sw_batch_size, num_data=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate for each 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh temp/spleen*.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=10, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=20, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=30, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=40, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=50, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=60, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=70, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=80, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=90, num_data=0)\n",
    "inference_segment_epoch(root_dir, model, 'unet', roi_size, sw_batch_size, epoch=100, num_data=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we conducted the split segmentation task using UNet model in CT data using MONAI network class. Additionally, we looked at the resume method of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference with saved checkpoint\n",
    "\n",
    "In today's workshop, we only learned 10 epochs due to time constraints. It provides learning parameters that proceeded 700 epoch learning for model verification. The parameter file is stored in a folder named `saved`. 700 epoch The file name of the learned parameter is `spleen_model_0700.pth`.\n",
    "In 700 epoch learning, the parameter with the best DICE metric is saved in the file: `spleen_model_best.pth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_segment_epoch('saved', model, 'unet', roi_size, sw_batch_size, epoch=700, num_data=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_segment_best('saved', model, 'unet', roi_size, sw_batch_size, num_data=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution !!!\n",
    "### please shutdown all kernels with [Kernel] menu >  [Shutdown All Kernel]  before launch next notebook\n",
    "\n",
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02 ](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04  ](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment ](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline Next ](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "moani_bootcamp_2_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
